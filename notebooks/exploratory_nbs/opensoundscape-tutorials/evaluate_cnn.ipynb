{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c62ee27e",
   "metadata": {},
   "source": [
    "# This notebook replicates the CNN training cycle taken from the Opensoundscape tutorial. \n",
    "\n",
    "In this notebook we will load a cnn saved to disk and run it on a validation and test set, then run BirdNET on the same data to see how it performs. \n",
    "\n",
    "This notebook is run using an environment with tensorflow installed which allows us to download and use birdnet for inference. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d88b73-77d1-4c00-a83a-8466fd79e15e",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c9eee8-c65c-4df1-95d0-15dda341ee0a",
   "metadata": {},
   "source": [
    "### Import needed packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "972e3e01-c85f-415d-95cc-9b695332f738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the cnn module provides classes for training/predicting with various types of CNNs\n",
    "from opensoundscape import CNN\n",
    "\n",
    "#other utilities and packages\n",
    "import torch\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import random \n",
    "import subprocess\n",
    "from glob import glob\n",
    "import sklearn\n",
    "import opensoundscape as opso\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22adf5d6-403d-4a06-bc85-477cdc60ec07",
   "metadata": {},
   "source": [
    "### Set random seeds\n",
    "\n",
    "Set manual seeds for Pytorch and Python. These essentially \"fix\" the results of any stochastic steps in model training, ensuring that training results are reproducible. You probably don't want to do this when you actually train your model, but it's useful for debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68e09bd5-e86d-44e0-8ffa-0f8ee699c31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "random.seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c60bac-280a-4d72-80b6-2659f6ecd83d",
   "metadata": {},
   "source": [
    "### Download files\n",
    "\n",
    "Training a machine learning model requires some pre-labeled data. These data, in the form of audio recordings or spectrograms, are labeled with whether or not they contain the sound of the species of interest. \n",
    "\n",
    "These data can be obtained from online databases such as Xeno-Canto.org, or by labeling one's own ARU data using a program like Cornell's Raven sound analysis software. In this example we are using a set of annotated avian soundscape recordings that were annotated using the software Raven Pro 1.6.4 (Bioacoustics Research Program 2022):\n",
    "\n",
    "<blockquote><i>An annotated set of audio recordings of Eastern North American birds containing frequency, time, and species information. </i> Lauren M. Chronister,  Tessa A. Rhinehart,  Aidan Place,  Justin Kitzes.\n",
    "https://doi.org/10.1002/ecy.3329 \n",
    "</blockquote>\n",
    "\n",
    "These are the same data that are used by the annotation and preprocessing tutorials, so you can skip this step if you've already downloaded them there.\n",
    "\n",
    "Download the datasets to your current working directory and unzip them. You can do so by running the cell below OR\n",
    "\n",
    "- Download and unzip both `annotation_Files.zip` and `mp3_Files.zip` from the https://datadryad.org/stash/dataset/doi:10.5061/dryad.d2547d81z  \n",
    "- Move the unzipped contents into a subfolder of the current folder called `./annotated_data/`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82705d0a-f5f7-4104-8ea7-461ca7f72e4e",
   "metadata": {},
   "source": [
    "## Prepare audio data\n",
    "\n",
    "To prepare audio data for machine learning, we need to convert our annotated data into clip-level labels.\n",
    "\n",
    "These steps are covered in depth in other tutorials, so we'll just set our clip labels up quickly for this example.\n",
    "\n",
    "First, get exactly matched lists of audio files and their corresponding selection files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61cbd28e-1e20-4709-95e7-dadf7f8b3f2c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['annotated_data/Annotation_Files/Recording_1/Recording_1_Segment_31.Table.1.selections.txt',\n",
       " 'annotated_data/Annotation_Files/Recording_1/Recording_1_Segment_07.Table.1.selections.txt',\n",
       " 'annotated_data/Annotation_Files/Recording_1/Recording_1_Segment_36.Table.1.selections.txt',\n",
       " 'annotated_data/Annotation_Files/Recording_1/Recording_1_Segment_04.Table.1.selections.txt',\n",
       " 'annotated_data/Annotation_Files/Recording_1/Recording_1_Segment_35.Table.1.selections.txt']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the current directory to where the dataset is downloaded\n",
    "dataset_path = Path(\"./annotated_data/\")\n",
    "\n",
    "# Make a list of all of the selection table files\n",
    "selection_files = glob(f\"{dataset_path}/Annotation_Files/*/*.txt\")\n",
    "selection_files[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cdb29706",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a list of audio files, one corresponding to each Raven file\n",
    "# (Audio files have the same names as selection files with a different extension)\n",
    "audio_files = [f.replace('Annotation_Files','Recordings').replace('.Table.1.selections.txt','.mp3') for f in selection_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c1fbbe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README.txt            test_set.csv          valid_set.csv\n",
      "\u001b[34mRecordings\u001b[m\u001b[m            train_and_val_set.csv \u001b[34mwav_files\u001b[m\u001b[m\n",
      "\u001b[34mannotation_Files\u001b[m\u001b[m      train_set.csv\n"
     ]
    }
   ],
   "source": [
    "!ls annotated_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc6709e-9508-4f08-b1ea-30d8662161b1",
   "metadata": {},
   "source": [
    "Next, convert the selection files and audio files to a `BoxedAnnotations` object, which contains the time, frequency, and label information for all annotations for every recording in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77f3f7a5-e074-4313-a1bd-6b5a4c98612e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mikeg/miniforge3/envs/osfltens/lib/python3.10/site-packages/opensoundscape/annotations.py:243: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  all_annotations = pd.concat(all_file_dfs).reset_index(drop=True)\n"
     ]
    }
   ],
   "source": [
    "from opensoundscape.annotations import BoxedAnnotations\n",
    "# Create a dataframe of annotations\n",
    "annotations = BoxedAnnotations.from_raven_files(\n",
    "    selection_files,\n",
    "    audio_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be86c1d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README.txt            test_set.csv          valid_set.csv\n",
      "\u001b[34mRecordings\u001b[m\u001b[m            train_and_val_set.csv \u001b[34mwav_files\u001b[m\u001b[m\n",
      "\u001b[34mannotation_Files\u001b[m\u001b[m      train_set.csv\n"
     ]
    }
   ],
   "source": [
    "!ls annotated_data/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35824d4",
   "metadata": {},
   "source": [
    "When extracting the downloaded database, there were two folders each containing the source recordings. These were the wav and mp3 folders. I had to renamne the mp3 folder to Recordings to match the path expected by this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b8c74cb-3fbf-4f29-8ed5-d62f51b645a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# # Parameters to use for label creation\n",
    "# clip_duration = 3.0\n",
    "# clip_overlap = 0.0\n",
    "# min_label_overlap = 0.25\n",
    "# species_of_interest = [\"NOCA\", \"EATO\", \"SCTA\", \"BAWW\", \"BCCH\", \"AMCR\", \"NOFL\"]\n",
    "\n",
    "# # Create dataframe of one-hot labels\n",
    "# clip_labels = annotations.one_hot_clip_labels(\n",
    "#     clip_duration = clip_duration, \n",
    "#     clip_overlap = clip_overlap,\n",
    "#     min_label_overlap = min_label_overlap,\n",
    "#     class_subset = species_of_interest # You can comment this line out if you want to include all species.\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7fe29b-d6e7-4593-9b44-813c5aafb00b",
   "metadata": {},
   "source": [
    "If you wanted, you could load the training and testing set from these saved CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81f53802-c25f-4cbe-ab7f-531b80f38cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_and_val_set = pd.read_csv('./annotated_data/train_and_val_set.csv',index_col=[0,1,2])\n",
    "# test_set = pd.read_csv('./annotated_data/test_set.csv',index_col=[0,1,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe66d592-fb5b-4e9d-a832-9ae123b9a442",
   "metadata": {},
   "source": [
    "### Load the CNN model we trained earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41696c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = opso.cnn.load_model('opso_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b5917f",
   "metadata": {},
   "source": [
    "### load the test and validation sets from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83c252fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = pd.read_csv('./annotated_data/test_set.csv',index_col=[0,1,2])\n",
    "valid_df = pd.read_csv('./annotated_data/valid_set.csv',index_col=[0,1,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92a8de1-3d6b-4f03-bd61-dae8c17f1ddf",
   "metadata": {},
   "source": [
    "### Check model device\n",
    "\n",
    "If a GPU is available on your computer, the CNN object automatically selects it for accellerating performance. You can override `.device` to use a specific device such as `cpu` or `cuda:3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9de0c6df-d999-4791-b358-312a076f6888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.device is: mps\n"
     ]
    }
   ],
   "source": [
    "print(f'model.device is: {model.device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b9afcf-fb51-45e8-9450-bcb2d17bac97",
   "metadata": {},
   "source": [
    "training on mps (Apple Silicon GPU) requires PyTorch >= 2.1.0. If we have an older "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "94dc7e68-b57c-4f8c-981c-d32d7c90d394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if model.device ==  torch.device('mps'):\n",
    "#     model.device=torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb3d185",
   "metadata": {},
   "source": [
    "## Let's take a look at the validation set used to evaluate this model during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e1742cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>NOCA</th>\n",
       "      <th>EATO</th>\n",
       "      <th>SCTA</th>\n",
       "      <th>BAWW</th>\n",
       "      <th>BCCH</th>\n",
       "      <th>AMCR</th>\n",
       "      <th>NOFL</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>file</th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>annotated_data/Recordings/Recording_2/Recording_2_Segment_09.mp3</th>\n",
       "      <th>75.0</th>\n",
       "      <th>78.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>annotated_data/Recordings/Recording_1/Recording_1_Segment_36.mp3</th>\n",
       "      <th>12.0</th>\n",
       "      <th>15.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>annotated_data/Recordings/Recording_2/Recording_2_Segment_03.mp3</th>\n",
       "      <th>60.0</th>\n",
       "      <th>63.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>annotated_data/Recordings/Recording_1/Recording_1_Segment_02.mp3</th>\n",
       "      <th>9.0</th>\n",
       "      <th>12.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>annotated_data/Recordings/Recording_2/Recording_2_Segment_10.mp3</th>\n",
       "      <th>9.0</th>\n",
       "      <th>12.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                        NOCA  \\\n",
       "file                                               start_time end_time         \n",
       "annotated_data/Recordings/Recording_2/Recording... 75.0       78.0       0.0   \n",
       "annotated_data/Recordings/Recording_1/Recording... 12.0       15.0       0.0   \n",
       "annotated_data/Recordings/Recording_2/Recording... 60.0       63.0       0.0   \n",
       "annotated_data/Recordings/Recording_1/Recording... 9.0        12.0       0.0   \n",
       "annotated_data/Recordings/Recording_2/Recording... 9.0        12.0       0.0   \n",
       "\n",
       "                                                                        EATO  \\\n",
       "file                                               start_time end_time         \n",
       "annotated_data/Recordings/Recording_2/Recording... 75.0       78.0       1.0   \n",
       "annotated_data/Recordings/Recording_1/Recording... 12.0       15.0       0.0   \n",
       "annotated_data/Recordings/Recording_2/Recording... 60.0       63.0       0.0   \n",
       "annotated_data/Recordings/Recording_1/Recording... 9.0        12.0       0.0   \n",
       "annotated_data/Recordings/Recording_2/Recording... 9.0        12.0       1.0   \n",
       "\n",
       "                                                                        SCTA  \\\n",
       "file                                               start_time end_time         \n",
       "annotated_data/Recordings/Recording_2/Recording... 75.0       78.0       0.0   \n",
       "annotated_data/Recordings/Recording_1/Recording... 12.0       15.0       0.0   \n",
       "annotated_data/Recordings/Recording_2/Recording... 60.0       63.0       0.0   \n",
       "annotated_data/Recordings/Recording_1/Recording... 9.0        12.0       0.0   \n",
       "annotated_data/Recordings/Recording_2/Recording... 9.0        12.0       0.0   \n",
       "\n",
       "                                                                        BAWW  \\\n",
       "file                                               start_time end_time         \n",
       "annotated_data/Recordings/Recording_2/Recording... 75.0       78.0       1.0   \n",
       "annotated_data/Recordings/Recording_1/Recording... 12.0       15.0       0.0   \n",
       "annotated_data/Recordings/Recording_2/Recording... 60.0       63.0       0.0   \n",
       "annotated_data/Recordings/Recording_1/Recording... 9.0        12.0       0.0   \n",
       "annotated_data/Recordings/Recording_2/Recording... 9.0        12.0       0.0   \n",
       "\n",
       "                                                                        BCCH  \\\n",
       "file                                               start_time end_time         \n",
       "annotated_data/Recordings/Recording_2/Recording... 75.0       78.0       0.0   \n",
       "annotated_data/Recordings/Recording_1/Recording... 12.0       15.0       0.0   \n",
       "annotated_data/Recordings/Recording_2/Recording... 60.0       63.0       1.0   \n",
       "annotated_data/Recordings/Recording_1/Recording... 9.0        12.0       0.0   \n",
       "annotated_data/Recordings/Recording_2/Recording... 9.0        12.0       0.0   \n",
       "\n",
       "                                                                        AMCR  \\\n",
       "file                                               start_time end_time         \n",
       "annotated_data/Recordings/Recording_2/Recording... 75.0       78.0       1.0   \n",
       "annotated_data/Recordings/Recording_1/Recording... 12.0       15.0       0.0   \n",
       "annotated_data/Recordings/Recording_2/Recording... 60.0       63.0       0.0   \n",
       "annotated_data/Recordings/Recording_1/Recording... 9.0        12.0       0.0   \n",
       "annotated_data/Recordings/Recording_2/Recording... 9.0        12.0       1.0   \n",
       "\n",
       "                                                                        NOFL  \n",
       "file                                               start_time end_time        \n",
       "annotated_data/Recordings/Recording_2/Recording... 75.0       78.0       0.0  \n",
       "annotated_data/Recordings/Recording_1/Recording... 12.0       15.0       0.0  \n",
       "annotated_data/Recordings/Recording_2/Recording... 60.0       63.0       0.0  \n",
       "annotated_data/Recordings/Recording_1/Recording... 9.0        12.0       0.0  \n",
       "annotated_data/Recordings/Recording_2/Recording... 9.0        12.0       0.0  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2003881",
   "metadata": {},
   "source": [
    "The validation set contains a random sample of clips from areas 1, 2 and 3 - so the model has never seen these clips before, but it has seen clips from the same location. Let's see how the model performs on these clips compared with the ones in the withheld test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b5db858e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bad0dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f544ff6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9a1f2e69bcb4f568aae4730814c65bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mikeg/miniforge3/envs/osfltens/lib/python3.10/site-packages/opensoundscape/sample.py:153: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"labels\": torch.tensor([s.labels for s in samples]),\n",
      "/Users/mikeg/miniforge3/envs/osfltens/lib/python3.10/site-packages/opensoundscape/sample.py:153: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"labels\": torch.tensor([s.labels for s in samples]),\n",
      "/Users/mikeg/miniforge3/envs/osfltens/lib/python3.10/site-packages/opensoundscape/sample.py:153: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"labels\": torch.tensor([s.labels for s in samples]),\n",
      "/Users/mikeg/miniforge3/envs/osfltens/lib/python3.10/site-packages/opensoundscape/sample.py:153: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"labels\": torch.tensor([s.labels for s in samples]),\n",
      "/Users/mikeg/miniforge3/envs/osfltens/lib/python3.10/site-packages/opensoundscape/sample.py:153: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"labels\": torch.tensor([s.labels for s in samples]),\n",
      "/Users/mikeg/miniforge3/envs/osfltens/lib/python3.10/site-packages/opensoundscape/sample.py:153: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"labels\": torch.tensor([s.labels for s in samples]),\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m warnings\u001b[38;5;241m.\u001b[39mcatch_warnings():\n\u001b[1;32m      4\u001b[0m     warnings\u001b[38;5;241m.\u001b[39msimplefilter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m, category\u001b[38;5;241m=\u001b[39m\u001b[38;5;167;01mFutureWarning\u001b[39;00m)\n\u001b[0;32m----> 6\u001b[0m     valid_preds \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalid_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     test_preds \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(test_set, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m)\n",
      "File \u001b[0;32m~/miniforge3/envs/osfltens/lib/python3.10/site-packages/opensoundscape/ml/cnn.py:231\u001b[0m, in \u001b[0;36mBaseClassifier.predict\u001b[0;34m(self, samples, batch_size, num_workers, activation_layer, split_files_into_clips, overlap_fraction, final_clip, bypass_augmentations, invalid_samples_log, raise_errors, wandb_session, return_invalid_samples, progress_bar, **kwargs)\u001b[0m\n\u001b[1;32m    220\u001b[0m     wandb_session\u001b[38;5;241m.\u001b[39mlog(\n\u001b[1;32m    221\u001b[0m         {\n\u001b[1;32m    222\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSamples / Preprocessed samples\u001b[39m\u001b[38;5;124m\"\u001b[39m: wandb_table(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    226\u001b[0m         }\n\u001b[1;32m    227\u001b[0m     )\n\u001b[1;32m    229\u001b[0m \u001b[38;5;66;03m### Prediction/Inference ###\u001b[39;00m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;66;03m# iterate dataloader and run inference (forward pass) to generate scores\u001b[39;00m\n\u001b[0;32m--> 231\u001b[0m pred_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwandb_session\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;66;03m### Apply activation layer ### #TODO: test speed vs. doing it in __call__ on batches\u001b[39;00m\n\u001b[1;32m    234\u001b[0m pred_scores \u001b[38;5;241m=\u001b[39m apply_activation_layer(pred_scores, activation_layer)\n",
      "File \u001b[0;32m~/miniforge3/envs/osfltens/lib/python3.10/site-packages/opensoundscape/ml/cnn.py:1135\u001b[0m, in \u001b[0;36mCNN.__call__\u001b[0;34m(self, dataloader, wandb_session, progress_bar)\u001b[0m\n\u001b[1;32m   1133\u001b[0m \u001b[38;5;66;03m# disable gradient updates during inference\u001b[39;00m\n\u001b[1;32m   1134\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m-> 1135\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, samples \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tqdm(dataloader, disable\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m progress_bar)):\n\u001b[1;32m   1136\u001b[0m         \u001b[38;5;66;03m# load a batch of images and labels from the  dataloader\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m         \u001b[38;5;66;03m# we collate here rather than in the DataLoader so that\u001b[39;00m\n\u001b[1;32m   1138\u001b[0m         \u001b[38;5;66;03m# we can still access the AudioSamples and thier information\u001b[39;00m\n\u001b[1;32m   1139\u001b[0m         batch_data \u001b[38;5;241m=\u001b[39m collate_audio_samples_to_dict(samples)\n\u001b[1;32m   1140\u001b[0m         batch_tensors \u001b[38;5;241m=\u001b[39m batch_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msamples\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[0;32m~/miniforge3/envs/osfltens/lib/python3.10/site-packages/tqdm/notebook.py:249\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    248\u001b[0m     it \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m(tqdm_notebook, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__iter__\u001b[39m()\n\u001b[0;32m--> 249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m it:\n\u001b[1;32m    250\u001b[0m         \u001b[38;5;66;03m# return super(tqdm...) will not catch exception\u001b[39;00m\n\u001b[1;32m    251\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m    252\u001b[0m \u001b[38;5;66;03m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/osfltens/lib/python3.10/site-packages/tqdm/std.py:1182\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1181\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1182\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1185\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/osfltens/lib/python3.10/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniforge3/envs/osfltens/lib/python3.10/site-packages/torch/utils/data/dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    677\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 678\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    680\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniforge3/envs/osfltens/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/miniforge3/envs/osfltens/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/miniforge3/envs/osfltens/lib/python3.10/site-packages/opensoundscape/ml/safe_dataset.py:164\u001b[0m, in \u001b[0;36mSafeDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    162\u001b[0m sample_or_exc \u001b[38;5;241m=\u001b[39m \u001b[38;5;167;01mException\u001b[39;00m  \u001b[38;5;66;03m# placeholder\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m attempts \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset):\n\u001b[0;32m--> 164\u001b[0m     sample_or_exc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_safe_get_item\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(sample_or_exc, \u001b[38;5;167;01mException\u001b[39;00m):\n\u001b[1;32m    166\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m sample_or_exc\n",
      "File \u001b[0;32m~/miniforge3/envs/osfltens/lib/python3.10/site-packages/opensoundscape/ml/safe_dataset.py:93\u001b[0m, in \u001b[0;36mSafeDataset._safe_get_item\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     91\u001b[0m     invalid_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex exceeded end of self.dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 93\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m idx \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_valid_indices:\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_valid_indices\u001b[38;5;241m.\u001b[39mappend(idx)\n",
      "File \u001b[0;32m~/miniforge3/envs/osfltens/lib/python3.10/site-packages/opensoundscape/ml/datasets.py:103\u001b[0m, in \u001b[0;36mAudioFileDataset.__getitem__\u001b[0;34m(self, idx, break_on_key, break_on_type)\u001b[0m\n\u001b[1;32m    100\u001b[0m sample \u001b[38;5;241m=\u001b[39m AudioSample\u001b[38;5;241m.\u001b[39mfrom_series(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_df\u001b[38;5;241m.\u001b[39miloc[idx])\n\u001b[1;32m    102\u001b[0m \u001b[38;5;66;03m# preprocessor.forward will raise PreprocessingError if something fails\u001b[39;00m\n\u001b[0;32m--> 103\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbypass_augmentations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbypass_augmentations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbreak_on_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbreak_on_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbreak_on_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbreak_on_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sample\n",
      "File \u001b[0;32m~/miniforge3/envs/osfltens/lib/python3.10/site-packages/opensoundscape/preprocess/preprocessors.py:149\u001b[0m, in \u001b[0;36mBasePreprocessor.forward\u001b[0;34m(self, sample, break_on_type, break_on_key, bypass_augmentations, trace)\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;66;03m# perform the action (modifies the AudioSample in-place)\u001b[39;00m\n\u001b[0;32m--> 149\u001b[0m \u001b[43maction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgo\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trace:  \u001b[38;5;66;03m# user requested record of preprocessing steps\u001b[39;00m\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;66;03m# save the current state of the sample's data\u001b[39;00m\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;66;03m# (trace is a Series with index matching self.pipeline)\u001b[39;00m\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniforge3/envs/osfltens/lib/python3.10/site-packages/opensoundscape/preprocess/actions.py:248\u001b[0m, in \u001b[0;36mSpectrogramToTensor.go\u001b[0;34m(self, sample)\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"converts sample.data from Spectrogram to Tensor\"\"\"\u001b[39;00m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;66;03m# sample.data must be Spectrogram object\u001b[39;00m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;66;03m# sample should have attributes: height, width, channels\u001b[39;00m\n\u001b[0;32m--> 248\u001b[0m sample\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m \u001b[43msample\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_image\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43msample\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwidth\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchannels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchannels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtorch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolormap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolormap\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43minvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minvert\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/osfltens/lib/python3.10/site-packages/opensoundscape/spectrogram.py:594\u001b[0m, in \u001b[0;36mSpectrogram.to_image\u001b[0;34m(self, shape, channels, colormap, invert, return_type, range)\u001b[0m\n\u001b[1;32m    592\u001b[0m         shape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mshape(array)[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    593\u001b[0m out_shape \u001b[38;5;241m=\u001b[39m [shape[\u001b[38;5;241m0\u001b[39m], shape[\u001b[38;5;241m1\u001b[39m], channels]\n\u001b[0;32m--> 594\u001b[0m array \u001b[38;5;241m=\u001b[39m \u001b[43mskimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_shape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    596\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpil\u001b[39m\u001b[38;5;124m\"\u001b[39m:  \u001b[38;5;66;03m# expected shape of input is [h,w,c]\u001b[39;00m\n\u001b[1;32m    597\u001b[0m     \u001b[38;5;66;03m# use correct type for PIL.Image, and scale from 0-1 to 0-255\u001b[39;00m\n\u001b[1;32m    598\u001b[0m     array \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39muint8(array \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m255\u001b[39m)\n",
      "File \u001b[0;32m~/miniforge3/envs/osfltens/lib/python3.10/site-packages/skimage/transform/_warps.py:185\u001b[0m, in \u001b[0;36mresize\u001b[0;34m(image, output_shape, order, mode, cval, clip, preserve_range, anti_aliasing, anti_aliasing_sigma)\u001b[0m\n\u001b[1;32m    182\u001b[0m     filtered \u001b[38;5;241m=\u001b[39m image\n\u001b[1;32m    184\u001b[0m zoom_factors \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m factors]\n\u001b[0;32m--> 185\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mndi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzoom\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfiltered\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzoom_factors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mndi_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m               \u001b[49m\u001b[43mcval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrid_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    188\u001b[0m _clip_warp_output(image, out, mode, cval, clip)\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/miniforge3/envs/osfltens/lib/python3.10/site-packages/scipy/ndimage/_interpolation.py:819\u001b[0m, in \u001b[0;36mzoom\u001b[0;34m(input, zoom, output, order, mode, cval, prefilter, grid_mode)\u001b[0m\n\u001b[1;32m    815\u001b[0m zoom \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39mdivide(zoom_nominator, zoom_div,\n\u001b[1;32m    816\u001b[0m                     out\u001b[38;5;241m=\u001b[39mnumpy\u001b[38;5;241m.\u001b[39mones_like(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mshape, dtype\u001b[38;5;241m=\u001b[39mnumpy\u001b[38;5;241m.\u001b[39mfloat64),\n\u001b[1;32m    817\u001b[0m                     where\u001b[38;5;241m=\u001b[39mzoom_div \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    818\u001b[0m zoom \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39mascontiguousarray(zoom)\n\u001b[0;32m--> 819\u001b[0m \u001b[43m_nd_image\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzoom_shift\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfiltered\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzoom\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnpad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    820\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mgrid_mode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    821\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "valid_preds = model.predict(valid_df, batch_size=32)\n",
    "test_preds = model.predict(test_set, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9ab209",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea1a1c8eddec4e2eb8a2339eaae426aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mikeg/miniforge3/envs/osfltens/lib/python3.10/site-packages/opensoundscape/sample.py:153: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"labels\": torch.tensor([s.labels for s in samples]),\n",
      "/Users/mikeg/miniforge3/envs/osfltens/lib/python3.10/site-packages/opensoundscape/sample.py:153: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"labels\": torch.tensor([s.labels for s in samples]),\n",
      "/Users/mikeg/miniforge3/envs/osfltens/lib/python3.10/site-packages/opensoundscape/sample.py:153: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"labels\": torch.tensor([s.labels for s in samples]),\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n",
      "\u001b[0;32m----> 1\u001b[0m valid_preds \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalid_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m      2\u001b[0m test_preds \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(test_set, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m)\n",
      "\n",
      "File \u001b[0;32m~/miniforge3/envs/osfltens/lib/python3.10/site-packages/opensoundscape/ml/cnn.py:231\u001b[0m, in \u001b[0;36mBaseClassifier.predict\u001b[0;34m(self, samples, batch_size, num_workers, activation_layer, split_files_into_clips, overlap_fraction, final_clip, bypass_augmentations, invalid_samples_log, raise_errors, wandb_session, return_invalid_samples, progress_bar, **kwargs)\u001b[0m\n",
      "\u001b[1;32m    220\u001b[0m     wandb_session\u001b[38;5;241m.\u001b[39mlog(\n",
      "\u001b[1;32m    221\u001b[0m         {\n",
      "\u001b[1;32m    222\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSamples / Preprocessed samples\u001b[39m\u001b[38;5;124m\"\u001b[39m: wandb_table(\n",
      "\u001b[0;32m   (...)\u001b[0m\n",
      "\u001b[1;32m    226\u001b[0m         }\n",
      "\u001b[1;32m    227\u001b[0m     )\n",
      "\u001b[1;32m    229\u001b[0m \u001b[38;5;66;03m### Prediction/Inference ###\u001b[39;00m\n",
      "\u001b[1;32m    230\u001b[0m \u001b[38;5;66;03m# iterate dataloader and run inference (forward pass) to generate scores\u001b[39;00m\n",
      "\u001b[0;32m--> 231\u001b[0m pred_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwandb_session\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    233\u001b[0m \u001b[38;5;66;03m### Apply activation layer ### #TODO: test speed vs. doing it in __call__ on batches\u001b[39;00m\n",
      "\u001b[1;32m    234\u001b[0m pred_scores \u001b[38;5;241m=\u001b[39m apply_activation_layer(pred_scores, activation_layer)\n",
      "\n",
      "File \u001b[0;32m~/miniforge3/envs/osfltens/lib/python3.10/site-packages/opensoundscape/ml/cnn.py:1135\u001b[0m, in \u001b[0;36mCNN.__call__\u001b[0;34m(self, dataloader, wandb_session, progress_bar)\u001b[0m\n",
      "\u001b[1;32m   1133\u001b[0m \u001b[38;5;66;03m# disable gradient updates during inference\u001b[39;00m\n",
      "\u001b[1;32m   1134\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "\u001b[0;32m-> 1135\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, samples \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tqdm(dataloader, disable\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m progress_bar)):\n",
      "\u001b[1;32m   1136\u001b[0m         \u001b[38;5;66;03m# load a batch of images and labels from the  dataloader\u001b[39;00m\n",
      "\u001b[1;32m   1137\u001b[0m         \u001b[38;5;66;03m# we collate here rather than in the DataLoader so that\u001b[39;00m\n",
      "\u001b[1;32m   1138\u001b[0m         \u001b[38;5;66;03m# we can still access the AudioSamples and thier information\u001b[39;00m\n",
      "\u001b[1;32m   1139\u001b[0m         batch_data \u001b[38;5;241m=\u001b[39m collate_audio_samples_to_dict(samples)\n",
      "\u001b[1;32m   1140\u001b[0m         batch_tensors \u001b[38;5;241m=\u001b[39m batch_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msamples\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n",
      "\n",
      "File \u001b[0;32m~/miniforge3/envs/osfltens/lib/python3.10/site-packages/tqdm/notebook.py:249\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;32m    248\u001b[0m     it \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m(tqdm_notebook, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__iter__\u001b[39m()\n",
      "\u001b[0;32m--> 249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m it:\n",
      "\u001b[1;32m    250\u001b[0m         \u001b[38;5;66;03m# return super(tqdm...) will not catch exception\u001b[39;00m\n",
      "\u001b[1;32m    251\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n",
      "\u001b[1;32m    252\u001b[0m \u001b[38;5;66;03m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m~/miniforge3/envs/osfltens/lib/python3.10/site-packages/tqdm/std.py:1182\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[1;32m   1179\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n",
      "\u001b[1;32m   1181\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;32m-> 1182\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n",
      "\u001b[1;32m   1183\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n",
      "\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n",
      "\u001b[1;32m   1185\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m~/miniforge3/envs/osfltens/lib/python3.10/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;32m    632\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n",
      "\u001b[1;32m    633\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n",
      "\u001b[0;32m--> 634\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    635\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n",
      "\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n",
      "\u001b[1;32m    638\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "\n",
      "File \u001b[0;32m~/miniforge3/envs/osfltens/lib/python3.10/site-packages/torch/utils/data/dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[1;32m    676\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n",
      "\u001b[1;32m    677\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n",
      "\u001b[0;32m--> 678\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n",
      "\u001b[1;32m    679\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n",
      "\u001b[1;32m    680\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "\n",
      "File \u001b[0;32m~/miniforge3/envs/osfltens/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n",
      "\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n",
      "\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "\n",
      "File \u001b[0;32m~/miniforge3/envs/osfltens/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n",
      "\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n",
      "\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "\n",
      "File \u001b[0;32m~/miniforge3/envs/osfltens/lib/python3.10/site-packages/opensoundscape/ml/safe_dataset.py:164\u001b[0m, in \u001b[0;36mSafeDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n",
      "\u001b[1;32m    162\u001b[0m sample_or_exc \u001b[38;5;241m=\u001b[39m \u001b[38;5;167;01mException\u001b[39;00m  \u001b[38;5;66;03m# placeholder\u001b[39;00m\n",
      "\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m attempts \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset):\n",
      "\u001b[0;32m--> 164\u001b[0m     sample_or_exc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_safe_get_item\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(sample_or_exc, \u001b[38;5;167;01mException\u001b[39;00m):\n",
      "\u001b[1;32m    166\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m sample_or_exc\n",
      "\n",
      "File \u001b[0;32m~/miniforge3/envs/osfltens/lib/python3.10/site-packages/opensoundscape/ml/safe_dataset.py:93\u001b[0m, in \u001b[0;36mSafeDataset._safe_get_item\u001b[0;34m(self, idx)\u001b[0m\n",
      "\u001b[1;32m     91\u001b[0m     invalid_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;32m     92\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex exceeded end of self.dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;32m---> 93\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m idx \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_valid_indices:\n",
      "\u001b[1;32m     95\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_valid_indices\u001b[38;5;241m.\u001b[39mappend(idx)\n",
      "\n",
      "File \u001b[0;32m~/miniforge3/envs/osfltens/lib/python3.10/site-packages/opensoundscape/ml/datasets.py:103\u001b[0m, in \u001b[0;36mAudioFileDataset.__getitem__\u001b[0;34m(self, idx, break_on_key, break_on_type)\u001b[0m\n",
      "\u001b[1;32m    100\u001b[0m sample \u001b[38;5;241m=\u001b[39m AudioSample\u001b[38;5;241m.\u001b[39mfrom_series(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_df\u001b[38;5;241m.\u001b[39miloc[idx])\n",
      "\u001b[1;32m    102\u001b[0m \u001b[38;5;66;03m# preprocessor.forward will raise PreprocessingError if something fails\u001b[39;00m\n",
      "\u001b[0;32m--> 103\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[1;32m    104\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    105\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbypass_augmentations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbypass_augmentations\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    106\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbreak_on_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbreak_on_key\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    107\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbreak_on_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbreak_on_type\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    108\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sample\n",
      "\n",
      "File \u001b[0;32m~/miniforge3/envs/osfltens/lib/python3.10/site-packages/opensoundscape/preprocess/preprocessors.py:149\u001b[0m, in \u001b[0;36mBasePreprocessor.forward\u001b[0;34m(self, sample, break_on_type, break_on_key, bypass_augmentations, trace)\u001b[0m\n",
      "\u001b[1;32m    146\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[1;32m    148\u001b[0m \u001b[38;5;66;03m# perform the action (modifies the AudioSample in-place)\u001b[39;00m\n",
      "\u001b[0;32m--> 149\u001b[0m \u001b[43maction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgo\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trace:  \u001b[38;5;66;03m# user requested record of preprocessing steps\u001b[39;00m\n",
      "\u001b[1;32m    152\u001b[0m     \u001b[38;5;66;03m# save the current state of the sample's data\u001b[39;00m\n",
      "\u001b[1;32m    153\u001b[0m     \u001b[38;5;66;03m# (trace is a Series with index matching self.pipeline)\u001b[39;00m\n",
      "\u001b[1;32m    154\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\n",
      "File \u001b[0;32m~/miniforge3/envs/osfltens/lib/python3.10/site-packages/opensoundscape/preprocess/actions.py:248\u001b[0m, in \u001b[0;36mSpectrogramToTensor.go\u001b[0;34m(self, sample)\u001b[0m\n",
      "\u001b[1;32m    245\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"converts sample.data from Spectrogram to Tensor\"\"\"\u001b[39;00m\n",
      "\u001b[1;32m    246\u001b[0m \u001b[38;5;66;03m# sample.data must be Spectrogram object\u001b[39;00m\n",
      "\u001b[1;32m    247\u001b[0m \u001b[38;5;66;03m# sample should have attributes: height, width, channels\u001b[39;00m\n",
      "\u001b[0;32m--> 248\u001b[0m sample\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m \u001b[43msample\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_image\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[1;32m    249\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43msample\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwidth\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    250\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchannels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchannels\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    251\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtorch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolormap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolormap\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43minvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minvert\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    254\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m~/miniforge3/envs/osfltens/lib/python3.10/site-packages/opensoundscape/spectrogram.py:594\u001b[0m, in \u001b[0;36mSpectrogram.to_image\u001b[0;34m(self, shape, channels, colormap, invert, return_type, range)\u001b[0m\n",
      "\u001b[1;32m    592\u001b[0m         shape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mshape(array)[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[1;32m    593\u001b[0m out_shape \u001b[38;5;241m=\u001b[39m [shape[\u001b[38;5;241m0\u001b[39m], shape[\u001b[38;5;241m1\u001b[39m], channels]\n",
      "\u001b[0;32m--> 594\u001b[0m array \u001b[38;5;241m=\u001b[39m \u001b[43mskimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_shape\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    596\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpil\u001b[39m\u001b[38;5;124m\"\u001b[39m:  \u001b[38;5;66;03m# expected shape of input is [h,w,c]\u001b[39;00m\n",
      "\u001b[1;32m    597\u001b[0m     \u001b[38;5;66;03m# use correct type for PIL.Image, and scale from 0-1 to 0-255\u001b[39;00m\n",
      "\u001b[1;32m    598\u001b[0m     array \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39muint8(array \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m255\u001b[39m)\n",
      "\n",
      "File \u001b[0;32m~/miniforge3/envs/osfltens/lib/python3.10/site-packages/skimage/transform/_warps.py:185\u001b[0m, in \u001b[0;36mresize\u001b[0;34m(image, output_shape, order, mode, cval, clip, preserve_range, anti_aliasing, anti_aliasing_sigma)\u001b[0m\n",
      "\u001b[1;32m    182\u001b[0m     filtered \u001b[38;5;241m=\u001b[39m image\n",
      "\u001b[1;32m    184\u001b[0m zoom_factors \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m factors]\n",
      "\u001b[0;32m--> 185\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mndi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzoom\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfiltered\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzoom_factors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mndi_mode\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    186\u001b[0m \u001b[43m               \u001b[49m\u001b[43mcval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrid_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    188\u001b[0m _clip_warp_output(image, out, mode, cval, clip)\n",
      "\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\n",
      "File \u001b[0;32m~/miniforge3/envs/osfltens/lib/python3.10/site-packages/scipy/ndimage/_interpolation.py:819\u001b[0m, in \u001b[0;36mzoom\u001b[0;34m(input, zoom, output, order, mode, cval, prefilter, grid_mode)\u001b[0m\n",
      "\u001b[1;32m    815\u001b[0m zoom \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39mdivide(zoom_nominator, zoom_div,\n",
      "\u001b[1;32m    816\u001b[0m                     out\u001b[38;5;241m=\u001b[39mnumpy\u001b[38;5;241m.\u001b[39mones_like(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mshape, dtype\u001b[38;5;241m=\u001b[39mnumpy\u001b[38;5;241m.\u001b[39mfloat64),\n",
      "\u001b[1;32m    817\u001b[0m                     where\u001b[38;5;241m=\u001b[39mzoom_div \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[1;32m    818\u001b[0m zoom \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39mascontiguousarray(zoom)\n",
      "\u001b[0;32m--> 819\u001b[0m \u001b[43m_nd_image\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzoom_shift\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfiltered\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzoom\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnpad\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    820\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mgrid_mode\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    821\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "valid_preds = model.predict(valid_df, batch_size=32)\n",
    "test_preds = model.predict(test_set, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd1da5e",
   "metadata": {},
   "source": [
    "# Measure performance on the validation and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7544ccb1",
   "metadata": {},
   "source": [
    "Multi target metrics on the validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfd7de5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "510"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_df[\"AMCR\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c226165",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>au_roc</th>\n",
       "      <th>avg_precision</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>valid</th>\n",
       "      <td>0.993753</td>\n",
       "      <td>0.991293</td>\n",
       "      <td>0.972414</td>\n",
       "      <td>0.946309</td>\n",
       "      <td>0.959184</td>\n",
       "      <td>149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_set</th>\n",
       "      <td>0.749758</td>\n",
       "      <td>0.191354</td>\n",
       "      <td>0.255319</td>\n",
       "      <td>0.122449</td>\n",
       "      <td>0.165517</td>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            au_roc  avg_precision  precision    recall        f1  support\n",
       "valid     0.993753       0.991293   0.972414  0.946309  0.959184      149\n",
       "test_set  0.749758       0.191354   0.255319  0.122449  0.165517      196"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop all columns except amcr\n",
    "# use multi target evaluation function then use the amcr column to get the AMCR\n",
    "\n",
    "valid_metrics = opso.metrics.multi_target_metrics(valid_df.values, valid_preds, valid_df.columns, threshold=0.5)['AMCR']\n",
    "test_set_metrics = opso.metrics.multi_target_metrics(test_set.values, test_preds, test_set.columns, threshold=0.5)['AMCR']\n",
    "\n",
    "pd.DataFrame([valid_metrics, test_set_metrics], index=['valid', 'test_set'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb39f2f",
   "metadata": {},
   "source": [
    "The scores are much lower on the withheld test set than on the validation set. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524c8193",
   "metadata": {},
   "source": [
    "$$\n",
    "precision = [\n",
    "\\frac{TP}{TP + FP}\n",
    "]\n",
    "$$ \n",
    "\n",
    "$$\n",
    "recall = [\n",
    "\\frac{TP}{TP + FN}\n",
    "]\n",
    "\n",
    "$$\n",
    "$$\n",
    "f1 = [\n",
    "\\frac{2 * precision * recall}{precision + recall}\n",
    "]\n",
    "\n",
    "\n",
    "$$\n",
    "$$\n",
    "accuracy = [\n",
    "\\frac{TP + TN}{TP + TN + FP + FN}\n",
    "]\n",
    "\n",
    "\n",
    "$$\n",
    "$$\n",
    "specificity = [\n",
    "\\frac{TN}{TN + FP}\n",
    "]\n",
    "$$\n",
    "\n",
    "$$ \n",
    "support = [\n",
    "TP + FN\n",
    "]\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964ce1a1",
   "metadata": {},
   "source": [
    "There's about 4600 clips in the training set and about 510 in the validation set for the AMRO for this example. The test set contains 196"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246d6060",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(510, 149, 2600, 196)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_df.AMCR), sum(valid_df.AMCR == 1), len(test_set.AMCR), sum(test_set.AMCR == 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021b9e1d",
   "metadata": {},
   "source": [
    "# do the other pretrained models do any better?\n",
    "See how BirdNET performs on this audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6dd986",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/mikeg/.cache/torch/hub/kitzeslab_bioacoustics-model-zoo_main\n",
      "Using cache found in /Users/mikeg/.cache/torch/hub/kitzeslab_bioacoustics-model-zoo_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading model from URL...\n",
      "Downloaded completed: BirdNET_GLOBAL_6K_V2.4_Model_FP16.tflite\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "WARNING: Attempting to use a delegate that only supports static-sized tensors with a graph that has dynamic-sized tensors (tensor#180 is a dynamic-sized tensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded completed: BirdNET_GLOBAL_6K_V2.4_Labels_af.txt\n"
     ]
    }
   ],
   "source": [
    "torch.hub.list('kitzeslab/bioacoustics-model-zoo')\n",
    "# All of these models require tensorflow, so I'll evaluate them in a separate notebook.\n",
    "birdnet = torch.hub.load('kitzeslab/bioacoustics-model-zoo', 'BirdNET')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3085d732",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Corvus brachyrhynchos_American Crow'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the location of the american crow\n",
    "birdnet_classes = birdnet.classes\n",
    "birdnet_classes[1575]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe24703f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mikeg/miniforge3/envs/osfltens/lib/python3.10/site-packages/opensoundscape/ml/cnn.py:201: UserWarning: The columns of input samples df differ from `model.classes`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49d02f92fc724972978ca3de7e7853a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# predict on validation set using birdnet\n",
    "birdnet_preds = birdnet.predict(valid_df, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e85d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def probs_from_logits(logits, threshold=0.5):\n",
    "    logits = torch.sigmoid(torch.tensor(logits))\n",
    "    return (logits > threshold).float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e1ef8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "opso_amcr_preds = valid_preds[\"AMCR\"].values\n",
    "birdnet_amcr_preds = birdnet_preds['Corvus brachyrhynchos_American Crow']\n",
    "\n",
    "opso_amcr_preds = probs_from_logits(opso_amcr_preds)\n",
    "birdnet_amcr_preds = probs_from_logits(birdnet_amcr_preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53c763e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.959731543624161, 0.959731543624161)"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Opso metrics on validation set\n",
    "opso_precision = opso.metrics.M.precision_score(valid_df[\"AMCR\"].values, opso_amcr_preds)\n",
    "opso_recall = opso.metrics.M.recall_score(valid_df[\"AMCR\"].values, opso_amcr_preds)\n",
    "opso_precision, opso_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6855c6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 0.8120805369127517)"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BirdNET metrics on validation set\n",
    "birdnet_precision = opso.metrics.M.precision_score(valid_df[\"AMCR\"].values, birdnet_amcr_preds)\n",
    "birdnet_recall = opso.metrics.M.recall_score(valid_df[\"AMCR\"].values, birdnet_amcr_preds)\n",
    "birdnet_precision, birdnet_recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4171e9bd",
   "metadata": {},
   "source": [
    "# Get metrics on the test set for both models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3595aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mikeg/miniforge3/envs/osfltens/lib/python3.10/site-packages/opensoundscape/ml/cnn.py:201: UserWarning: The columns of input samples df differ from `model.classes`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bc925cdb45c4d338f751ecc24bb4aee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/82 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# predict on test set using birdnet\n",
    "birdnet_test_logits = birdnet.predict(test_set, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0475d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hy/36n7jr6x3h74ddx507dc12_00000gn/T/ipykernel_30030/3723163624.py:2: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  logits = torch.sigmoid(torch.tensor(logits))\n"
     ]
    }
   ],
   "source": [
    "birdnet_test_probs = probs_from_logits(birdnet_test_logits['Corvus brachyrhynchos_American Crow'])\n",
    "birdnet_precision_testset = opso.metrics.M.precision_score(test_set[\"AMCR\"].values, birdnet_test_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bfb2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "birdnet_test_recall = opso.metrics.M.recall_score(test_set[\"AMCR\"].values, birdnet_test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523aa190",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4b8c2b595244846a38bb1f6d78046a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/82 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mikeg/miniforge3/envs/osfltens/lib/python3.10/site-packages/opensoundscape/sample.py:152: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"labels\": torch.Tensor([s.labels for s in samples]),\n",
      "/Users/mikeg/miniforge3/envs/osfltens/lib/python3.10/site-packages/opensoundscape/sample.py:152: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"labels\": torch.Tensor([s.labels for s in samples]),\n",
      "/Users/mikeg/miniforge3/envs/osfltens/lib/python3.10/site-packages/opensoundscape/sample.py:152: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"labels\": torch.Tensor([s.labels for s in samples]),\n",
      "/Users/mikeg/miniforge3/envs/osfltens/lib/python3.10/site-packages/opensoundscape/sample.py:152: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"labels\": torch.Tensor([s.labels for s in samples]),\n",
      "/Users/mikeg/miniforge3/envs/osfltens/lib/python3.10/site-packages/opensoundscape/sample.py:152: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"labels\": torch.Tensor([s.labels for s in samples]),\n",
      "/Users/mikeg/miniforge3/envs/osfltens/lib/python3.10/site-packages/opensoundscape/sample.py:152: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"labels\": torch.Tensor([s.labels for s in samples]),\n",
      "/Users/mikeg/miniforge3/envs/osfltens/lib/python3.10/site-packages/opensoundscape/sample.py:152: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"labels\": torch.Tensor([s.labels for s in samples]),\n",
      "/Users/mikeg/miniforge3/envs/osfltens/lib/python3.10/site-packages/opensoundscape/sample.py:152: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"labels\": torch.Tensor([s.labels for s in samples]),\n",
      "/Users/mikeg/miniforge3/envs/osfltens/lib/python3.10/site-packages/opensoundscape/sample.py:152: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"labels\": torch.Tensor([s.labels for s in samples]),\n",
      "/Users/mikeg/miniforge3/envs/osfltens/lib/python3.10/site-packages/opensoundscape/sample.py:152: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"labels\": torch.Tensor([s.labels for s in samples]),\n",
      "/Users/mikeg/miniforge3/envs/osfltens/lib/python3.10/site-packages/opensoundscape/sample.py:152: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"labels\": torch.Tensor([s.labels for s in samples]),\n",
      "/Users/mikeg/miniforge3/envs/osfltens/lib/python3.10/site-packages/opensoundscape/sample.py:152: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"labels\": torch.Tensor([s.labels for s in samples]),\n",
      "/Users/mikeg/miniforge3/envs/osfltens/lib/python3.10/site-packages/opensoundscape/sample.py:152: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"labels\": torch.Tensor([s.labels for s in samples]),\n",
      "/Users/mikeg/miniforge3/envs/osfltens/lib/python3.10/site-packages/opensoundscape/sample.py:152: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"labels\": torch.Tensor([s.labels for s in samples]),\n",
      "/Users/mikeg/miniforge3/envs/osfltens/lib/python3.10/site-packages/opensoundscape/sample.py:152: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"labels\": torch.Tensor([s.labels for s in samples]),\n",
      "/Users/mikeg/miniforge3/envs/osfltens/lib/python3.10/site-packages/opensoundscape/sample.py:152: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"labels\": torch.Tensor([s.labels for s in samples]),\n",
      "/Users/mikeg/miniforge3/envs/osfltens/lib/python3.10/site-packages/opensoundscape/sample.py:152: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"labels\": torch.Tensor([s.labels for s in samples]),\n",
      "/Users/mikeg/miniforge3/envs/osfltens/lib/python3.10/site-packages/opensoundscape/sample.py:152: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"labels\": torch.Tensor([s.labels for s in samples]),\n",
      "/Users/mikeg/miniforge3/envs/osfltens/lib/python3.10/site-packages/opensoundscape/sample.py:152: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"labels\": torch.Tensor([s.labels for s in samples]),\n",
      "/Users/mikeg/miniforge3/envs/osfltens/lib/python3.10/site-packages/opensoundscape/sample.py:152: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"labels\": torch.Tensor([s.labels for s in samples]),\n",
      "/Users/mikeg/miniforge3/envs/osfltens/lib/python3.10/site-packages/opensoundscape/sample.py:152: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"labels\": torch.Tensor([s.labels for s in samples]),\n",
      "/Users/mikeg/miniforge3/envs/osfltens/lib/python3.10/site-packages/opensoundscape/sample.py:152: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"labels\": torch.Tensor([s.labels for s in samples]),\n",
      "/Users/mikeg/miniforge3/envs/osfltens/lib/python3.10/site-packages/opensoundscape/sample.py:152: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"labels\": torch.Tensor([s.labels for s in samples]),\n",
      "/Users/mikeg/miniforge3/envs/osfltens/lib/python3.10/site-packages/opensoundscape/sample.py:152: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"labels\": torch.Tensor([s.labels for s in samples]),\n",
      "/Users/mikeg/miniforge3/envs/osfltens/lib/python3.10/site-packages/opensoundscape/sample.py:152: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"labels\": torch.Tensor([s.labels for s in samples]),\n",
      "/Users/mikeg/miniforge3/envs/osfltens/lib/python3.10/site-packages/opensoundscape/sample.py:152: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"labels\": torch.Tensor([s.labels for s in samples]),\n",
      "/Users/mikeg/miniforge3/envs/osfltens/lib/python3.10/site-packages/opensoundscape/sample.py:152: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"labels\": torch.Tensor([s.labels for s in samples]),\n",
      "/Users/mikeg/miniforge3/envs/osfltens/lib/python3.10/site-packages/opensoundscape/sample.py:152: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"labels\": torch.Tensor([s.labels for s in samples]),\n",
      "/Users/mikeg/miniforge3/envs/osfltens/lib/python3.10/site-packages/opensoundscape/sample.py:152: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"labels\": torch.Tensor([s.labels for s in samples]),\n",
      "/Users/mikeg/miniforge3/envs/osfltens/lib/python3.10/site-packages/opensoundscape/sample.py:152: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"labels\": torch.Tensor([s.labels for s in samples]),\n",
      "/Users/mikeg/miniforge3/envs/osfltens/lib/python3.10/site-packages/opensoundscape/sample.py:152: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"labels\": torch.Tensor([s.labels for s in samples]),\n",
      "/Users/mikeg/miniforge3/envs/osfltens/lib/python3.10/site-packages/opensoundscape/sample.py:152: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"labels\": torch.Tensor([s.labels for s in samples]),\n",
      "/Users/mikeg/miniforge3/envs/osfltens/lib/python3.10/site-packages/opensoundscape/sample.py:152: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"labels\": torch.Tensor([s.labels for s in samples]),\n",
      "/Users/mikeg/miniforge3/envs/osfltens/lib/python3.10/site-packages/opensoundscape/sample.py:152: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"labels\": torch.Tensor([s.labels for s in samples]),\n",
      "/Users/mikeg/miniforge3/envs/osfltens/lib/python3.10/site-packages/opensoundscape/sample.py:152: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"labels\": torch.Tensor([s.labels for s in samples]),\n",
      "/Users/mikeg/miniforge3/envs/osfltens/lib/python3.10/site-packages/opensoundscape/sample.py:152: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"labels\": torch.Tensor([s.labels for s in samples]),\n",
      "/Users/mikeg/miniforge3/envs/osfltens/lib/python3.10/site-packages/opensoundscape/sample.py:152: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"labels\": torch.Tensor([s.labels for s in samples]),\n",
      "/Users/mikeg/miniforge3/envs/osfltens/lib/python3.10/site-packages/opensoundscape/sample.py:152: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"labels\": torch.Tensor([s.labels for s in samples]),\n",
      "/Users/mikeg/miniforge3/envs/osfltens/lib/python3.10/site-packages/opensoundscape/sample.py:152: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"labels\": torch.Tensor([s.labels for s in samples]),\n",
      "/Users/mikeg/miniforge3/envs/osfltens/lib/python3.10/site-packages/opensoundscape/sample.py:152: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"labels\": torch.Tensor([s.labels for s in samples]),\n",
      "/Users/mikeg/miniforge3/envs/osfltens/lib/python3.10/site-packages/opensoundscape/sample.py:152: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"labels\": torch.Tensor([s.labels for s in samples]),\n",
      "/Users/mikeg/miniforge3/envs/osfltens/lib/python3.10/site-packages/opensoundscape/sample.py:152: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"labels\": torch.Tensor([s.labels for s in samples]),\n",
      "/Users/mikeg/miniforge3/envs/osfltens/lib/python3.10/site-packages/opensoundscape/sample.py:152: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"labels\": torch.Tensor([s.labels for s in samples]),\n",
      "/Users/mikeg/miniforge3/envs/osfltens/lib/python3.10/site-packages/opensoundscape/sample.py:152: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"labels\": torch.Tensor([s.labels for s in samples]),\n",
      "/Users/mikeg/miniforge3/envs/osfltens/lib/python3.10/site-packages/opensoundscape/sample.py:152: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"labels\": torch.Tensor([s.labels for s in samples]),\n",
      "/Users/mikeg/miniforge3/envs/osfltens/lib/python3.10/site-packages/opensoundscape/sample.py:152: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"labels\": torch.Tensor([s.labels for s in samples]),\n",
      "/Users/mikeg/miniforge3/envs/osfltens/lib/python3.10/site-packages/opensoundscape/sample.py:152: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"labels\": torch.Tensor([s.labels for s in samples]),\n",
      "/Users/mikeg/miniforge3/envs/osfltens/lib/python3.10/site-packages/opensoundscape/sample.py:152: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"labels\": torch.Tensor([s.labels for s in samples]),\n",
      "/Users/mikeg/miniforge3/envs/osfltens/lib/python3.10/site-packages/opensoundscape/sample.py:152: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"labels\": torch.Tensor([s.labels for s in samples]),\n",
      "/Users/mikeg/miniforge3/envs/osfltens/lib/python3.10/site-packages/opensoundscape/sample.py:152: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"labels\": torch.Tensor([s.labels for s in samples]),\n",
      "/Users/mikeg/miniforge3/envs/osfltens/lib/python3.10/site-packages/opensoundscape/sample.py:152: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"labels\": torch.Tensor([s.labels for s in samples]),\n",
      "/Users/mikeg/miniforge3/envs/osfltens/lib/python3.10/site-packages/opensoundscape/sample.py:152: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"labels\": torch.Tensor([s.labels for s in samples]),\n",
      "/Users/mikeg/miniforge3/envs/osfltens/lib/python3.10/site-packages/opensoundscape/sample.py:152: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"labels\": torch.Tensor([s.labels for s in samples]),\n",
      "/Users/mikeg/miniforge3/envs/osfltens/lib/python3.10/site-packages/opensoundscape/sample.py:152: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"labels\": torch.Tensor([s.labels for s in samples]),\n",
      "/Users/mikeg/miniforge3/envs/osfltens/lib/python3.10/site-packages/opensoundscape/sample.py:152: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"labels\": torch.Tensor([s.labels for s in samples]),\n",
      "/Users/mikeg/miniforge3/envs/osfltens/lib/python3.10/site-packages/opensoundscape/sample.py:152: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"labels\": torch.Tensor([s.labels for s in samples]),\n",
      "/Users/mikeg/miniforge3/envs/osfltens/lib/python3.10/site-packages/opensoundscape/sample.py:152: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"labels\": torch.Tensor([s.labels for s in samples]),\n",
      "/Users/mikeg/miniforge3/envs/osfltens/lib/python3.10/site-packages/opensoundscape/sample.py:152: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"labels\": torch.Tensor([s.labels for s in samples]),\n",
      "/Users/mikeg/miniforge3/envs/osfltens/lib/python3.10/site-packages/opensoundscape/sample.py:152: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"labels\": torch.Tensor([s.labels for s in samples]),\n",
      "/Users/mikeg/miniforge3/envs/osfltens/lib/python3.10/site-packages/opensoundscape/sample.py:152: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"labels\": torch.Tensor([s.labels for s in samples]),\n",
      "/Users/mikeg/miniforge3/envs/osfltens/lib/python3.10/site-packages/opensoundscape/sample.py:152: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"labels\": torch.Tensor([s.labels for s in samples]),\n",
      "/Users/mikeg/miniforge3/envs/osfltens/lib/python3.10/site-packages/opensoundscape/sample.py:152: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"labels\": torch.Tensor([s.labels for s in samples]),\n",
      "/Users/mikeg/miniforge3/envs/osfltens/lib/python3.10/site-packages/opensoundscape/sample.py:152: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"labels\": torch.Tensor([s.labels for s in samples]),\n",
      "/Users/mikeg/miniforge3/envs/osfltens/lib/python3.10/site-packages/opensoundscape/sample.py:152: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"labels\": torch.Tensor([s.labels for s in samples]),\n",
      "/Users/mikeg/miniforge3/envs/osfltens/lib/python3.10/site-packages/opensoundscape/sample.py:152: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"labels\": torch.Tensor([s.labels for s in samples]),\n",
      "/Users/mikeg/miniforge3/envs/osfltens/lib/python3.10/site-packages/opensoundscape/sample.py:152: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"labels\": torch.Tensor([s.labels for s in samples]),\n",
      "/Users/mikeg/miniforge3/envs/osfltens/lib/python3.10/site-packages/opensoundscape/sample.py:152: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"labels\": torch.Tensor([s.labels for s in samples]),\n",
      "/Users/mikeg/miniforge3/envs/osfltens/lib/python3.10/site-packages/opensoundscape/sample.py:152: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"labels\": torch.Tensor([s.labels for s in samples]),\n",
      "/Users/mikeg/miniforge3/envs/osfltens/lib/python3.10/site-packages/opensoundscape/sample.py:152: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"labels\": torch.Tensor([s.labels for s in samples]),\n",
      "/Users/mikeg/miniforge3/envs/osfltens/lib/python3.10/site-packages/opensoundscape/sample.py:152: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"labels\": torch.Tensor([s.labels for s in samples]),\n",
      "/Users/mikeg/miniforge3/envs/osfltens/lib/python3.10/site-packages/opensoundscape/sample.py:152: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"labels\": torch.Tensor([s.labels for s in samples]),\n",
      "/Users/mikeg/miniforge3/envs/osfltens/lib/python3.10/site-packages/opensoundscape/sample.py:152: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"labels\": torch.Tensor([s.labels for s in samples]),\n",
      "/Users/mikeg/miniforge3/envs/osfltens/lib/python3.10/site-packages/opensoundscape/sample.py:152: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"labels\": torch.Tensor([s.labels for s in samples]),\n",
      "/Users/mikeg/miniforge3/envs/osfltens/lib/python3.10/site-packages/opensoundscape/sample.py:152: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"labels\": torch.Tensor([s.labels for s in samples]),\n",
      "/Users/mikeg/miniforge3/envs/osfltens/lib/python3.10/site-packages/opensoundscape/sample.py:152: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"labels\": torch.Tensor([s.labels for s in samples]),\n",
      "/Users/mikeg/miniforge3/envs/osfltens/lib/python3.10/site-packages/opensoundscape/sample.py:152: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"labels\": torch.Tensor([s.labels for s in samples]),\n",
      "/Users/mikeg/miniforge3/envs/osfltens/lib/python3.10/site-packages/opensoundscape/sample.py:152: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"labels\": torch.Tensor([s.labels for s in samples]),\n",
      "/Users/mikeg/miniforge3/envs/osfltens/lib/python3.10/site-packages/opensoundscape/sample.py:152: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"labels\": torch.Tensor([s.labels for s in samples]),\n",
      "/Users/mikeg/miniforge3/envs/osfltens/lib/python3.10/site-packages/opensoundscape/sample.py:152: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"labels\": torch.Tensor([s.labels for s in samples]),\n",
      "/Users/mikeg/miniforge3/envs/osfltens/lib/python3.10/site-packages/opensoundscape/sample.py:152: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"labels\": torch.Tensor([s.labels for s in samples]),\n",
      "/Users/mikeg/miniforge3/envs/osfltens/lib/python3.10/site-packages/opensoundscape/sample.py:152: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"labels\": torch.Tensor([s.labels for s in samples]),\n",
      "/Users/mikeg/miniforge3/envs/osfltens/lib/python3.10/site-packages/opensoundscape/sample.py:152: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"labels\": torch.Tensor([s.labels for s in samples]),\n"
     ]
    }
   ],
   "source": [
    "# Get opso predicitions from test set\n",
    "opso_test_logits = model.predict(test_set, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2616863",
   "metadata": {},
   "outputs": [],
   "source": [
    "opso_probs = probs_from_logits(opso_test_logits[\"AMCR\"].values)\n",
    "opso_precision_testset = opso.metrics.M.precision_score(test_set[\"AMCR\"].values, opso_probs)\n",
    "opso_recall_testset = opso.metrics.M.recall_score(test_set[\"AMCR\"].values, opso_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbcaa9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9411764705882353, 0.08163265306122448, 0.272, 0.17346938775510204)"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "birdnet_precision_testset, birdnet_test_recall, opso_precision_testset, opso_recall_testset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14bc744",
   "metadata": {},
   "source": [
    "# Results are inconlusive\n",
    "It looks as though birdnet is better since it has higher precision, but the recall is lower at this threshold. More thresholds or f1 score should be caluclated to shed more light on this. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b498f89-d856-45b5-bfe6-b9e94e603ada",
   "metadata": {},
   "source": [
    "Once this is finished running, you have trained the CNN. \n",
    "\n",
    "**Clean up:** Run the following cell to delete the files created in this tutorial. However, these files are used in other tutorials, so you may wish not to delete them just yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440ca518-abcd-4bac-94e8-12ff8b8e46b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shutil\n",
    "# shutil.rmtree('./annotated_data')\n",
    "# shutil.rmtree('./wandb')\n",
    "# shutil.rmtree('./model_training_checkpoints')\n",
    "# Path('annotation_Files.zip').unlink()\n",
    "# Path('mp3_Files.zip').unlink()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "osfl2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
