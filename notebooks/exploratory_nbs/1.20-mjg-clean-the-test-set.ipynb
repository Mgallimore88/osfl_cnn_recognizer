{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "\n",
    "\n",
    "# Clean the contents of the test set to remove mis-labelled clips\n",
    "\n",
    "### General rules of thumb for all ML models\n",
    "- The dataset needs to contain true examples of calls and absences.\n",
    "This necessetates some cleaning of the human labels since some of them are mis-labelled. \n",
    "- The test set should represent the situation at inference as closely as possible, so that the measured metrics more closely match what is expected to be the case in the field. \n",
    "At inference time the audio will include a range of song volumes and clarity - all the way from focal and loud, to far away and masked and everything between.\n",
    "\n",
    "### Considerations for this project\n",
    "- The model needs to output a lower score for quieter calls - this is to enable density estimation and other downstream statistical applications. \n",
    "- To enable this, the __training__ set will contain focal samples, whilst the test set will contain a mixture of focal and distant samples. The assumption is that this will produce a model which outputs a lower score for quieter samples. \n",
    " \n",
    "\n",
    "### Cleaning samples from the test set introduces an interesting problem:\n",
    "There's a subjective threshold in the decision of whether to throw a sample out of the test set or not. Initially the test set contains a random sample of the human labelled OSFL calls. These will contain audio which spread the full range of human detection ability, plus some errors. \n",
    "\n",
    "\n",
    "In cleaning out the errors, I'll come across some which look like errors but which I'm not sure about, because to some degree they are masked or quiet. Throwing these out could be removing some hard edge cases from the test set, which could be useful for measuring increases in recall.\n",
    "\n",
    " To ensure that the test set contains only accurate labels, I could throw out all the samples except those which I'm 100% convinced are correct. But in doing so I'd make a test set which by definition, contains easy to recognize samples. This would have a direct affect on the model metrics. \n",
    "\n",
    "At the other extreme, I could do nothing - leave some errors in the test set, which would have the advantage of being closer to some notion of ground truth - these are the labels which a range of human labellers returned, plus some errors which put an upper limit on how high the metric measurements can get. \n",
    "The clarity of the samples left in the test set will directly affect the performance metric of a model tested on this test set. \n",
    "\n",
    "\n",
    "\n",
    "# Proposed solution:\n",
    "Rather than throw out items from the test set, I'll label them with confidence scores:\n",
    "- 0 is something I should throw out, \n",
    "- 1 is not sure, \n",
    "- 2 is definitely an olive sided flycatcher. \n",
    "- 3 (Optionally 3 is focal OSFL)\n",
    "\n",
    "This will let me measure performance on a test set which I know only contains verified labels, and optionally mix in on the hard to detect samples which might otherwise get thrown out.\n",
    "\n",
    "\n",
    "# Label the test set with confidence scores\n",
    "This part of the notebook \n",
    "\n",
    "- takes a test set saved to disk as an AudioFileDataset (or just a pandas dataframe)\n",
    "- loads each sample with playback option\n",
    "- takes a user typed label as input\n",
    "- saves the label in a column of the dataframe titled 'confidence' \n",
    "- saves the dataset back to disk\n",
    "\n"
=======
    "# Contents of a good test set\n",
    "\n",
    "- The test set should represent the situation at inference as closely as possible\n",
    "- The model needs to output a lower score for quieter calls - this is to enable density estimation and other downstream statistical applications. \n",
    "- To enable this, the test set will contain focal samples. \n",
    "- To test this behaviour, the test set will need to contain some quieter samples.\n",
    "\n",
    "\n",
    "### Cleaning samples from the test set introduces another level of subjectivity \n",
    "The clarity of the samples left in the test set will directly affect the performance metric of a model tested on this test set. \n",
    "\n",
    "Proposal:\n",
    "1. To clean out and discard definite errors\n",
    "2. To label uncertain sections of audio with a flag\n",
    "3. To keep the focal examples in the test.\n",
    "\n",
    "Optional:\n",
    "use postprocessing to augment the test set with decreasing volume samples. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mark the test set audio as focal, mistake, or uncertain"
>>>>>>> 5b2f271b69a0a68215a0d9a0a6065c00aba836ee
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
